# 集成学习

tags: 机器学习

---

https://www.jiqizhixin.com/articles/2018-07-28-3

## 集成学习一览

**组合多个弱监督模型以得到一个更好更全面的强监督模型，其思想在于：即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。**

集成方法奏效的原因是不同的模型**通常不会**在测试集上产生相同的误差。集成模型能至少与它的任一成员表现得一样好。**如果成员的误差是独立的**，集成将显著提升模型的性能。

学习策略推荐：

> - 数据集大： 划分成多个小数据集，学习多个模型进行组合。
> - 数据集小： 利用 Bootstrap 方法进行抽样，得到多个数据集，分别训练多个模型再进行组合。

## 1. Bagging

代表模型： **随机森林， Bagging meta-estimator**

先通过采样构造 k 个不同的数据集，学习得到k 个基学习器， 基学习器之间不存在依赖关系，可同时生成。    

更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 `2/3` 的实例

- **Bootstrap：**  一种有放回的抽样方法，目的是为了得到统计量的分布以及置信空间。

  > 1. 采用有放回抽样方法从原始样本中抽取一定数量的样本
  > 2. 根据抽出的样本计算想要得到的统计量T
  > 3. 重复上述N次（一般大于1000），得到N个统计量T
  > 4. 根据这N个统计量，即可计算出统计量的置信区间

- **Bagging 的基本思路：**

  > 1.  利用**Bootstrap**对训练集随机采样，重复进行 `T` 次
  > 2. 基于每个采样集训练一个弱学习器，得到 T 个弱学习器
  > 3. 预测时，分类问题采用投票方式， 回归问题采用 N 个模型预测平均方式。

## 2.  Boosting 

代表模型：**AdaBoost， XGBoost， GBDT， Light GBM， CatBoost**

学习一系列弱学习器，然后组合成一个强学习器。基于**串行策略**：弱学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。

- Boosting基本思路：

  > 1. 先从初始训练集训练一个弱学习器，初始训练集各个样本权重相同
  >
  > 2. 根据上一个弱学习器的表现，调整样本权重，是的分类错误的样本得到更多关注
  > 3. 基于调整后的样本分布，训练下一个弱学习器、
  > 4. 测试时，对各基学习器**加权**得到最终结果

## 3. Stacking

训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。

- Stacking 基本思路：首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。

  > - 先从初始训练集训练 `T` 个**不同的初级学习器**;
  > - 利用每个初级学习器的**输出**构建一个**次级数据集**，该数据集依然使用初始数据集的标签；
  > - 根据新的数据集训练**次级学习器**；
  > - **多级学习器**的构建过程类似。



---

## QA

### 0. Boosting 与 Bagging 区别

- Bagging中每个训练集互不相关，也就是每个基分类器互不相关，而Boosting中训练集要在上一轮的结果上进行调整，也使得其不能并行计算
- Bagging中预测函数是均匀平等的，但在Boosting中预测函数是加权的

### 1. Boosting, Bagging 与偏差，方差

**Boosting** 能提升弱分类器性能的原因是降低了**偏差**；**Bagging** 则是降低了**方差**；

- Boosting：

  > - Boosting 的**基本思路**就是在不断减小模型的**训练误差**（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差；
  > - 但 Boosting 不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。

- Bagging：

  > - 对 `n` 个**独立不相关的模型**预测结果取平均，方差是原来的 `1/n`；
  > - 假设所有基分类器出错的概率是独立的，**超过半数**基分类器出错的概率会随着基分类器的数量增加而下降。