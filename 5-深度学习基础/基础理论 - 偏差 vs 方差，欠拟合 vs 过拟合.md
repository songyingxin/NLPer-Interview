# 偏差 vs 方差

tags: 机器学习

---

参考： [偏差与方差， 欠拟合与过拟合](https://zhuanlan.zhihu.com/p/48257326)

## 定义

- 记在**训练集 D** 上学得的模型为

  $$
  f(x;D)
  $$


  模型的**期望预测**为
  $$
  \hat{f}(x) = E_D [f(x;D)]
  $$

- **偏差**（Bias）
  $$
  bias^2(x) = (\hat{f}(x) - y)^2
  $$

  > **偏差**度量了学习算法的**期望预测**与**真实结果**的偏离程度，即刻画了学习算法本身的拟合能力；

- **方差**（Variance）
  $$
  var(x) = E_D[(f(x;D) - \hat{f}(x))^2]
  $$


  > **方差**度量了同样大小的**训练集的变动**所导致的学习性能的变化，即刻画了**数据扰动所造成的影响（模型的稳定性**）；

  $$
  \varepsilon^2 = E_D[(y_D-y)^2]
  $$

- **噪声**则表达了**在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。**

- “**偏差-方差分解**”表明模型的泛化能力是由算法的能力、数据的充分性、任务本身的难度共同决定的。

## 理解概念

- 偏差： 表示模型在训练集上的表现，与**训练误差**成线性关系， 用于描述模型的拟合能力
- 方差： 表示模型在（开发集或测试集）与**测试误差-训练误差**成线性关系，用于描述模型的泛化能力。

## 四种情况

1. 偏差很低，方差很高： 意味着训练误差很低，测试误差很高，此时发生了过拟合现象。
2. 偏差很高，方差很低： 意味着训练误差，测试误差都很高，此时发生了欠拟合现在。
3. 偏差，方差都很高： 意味着此时同时发生了欠拟合和过拟合现象。
4. 偏差很低，方差很低： 意味着训练误差很低，测试误差也很低，表示我们的模型训练的结果很好。

## 偏差与欠拟合，方差与过拟合

- 偏差通常是由于我们定义的模型不合适或模型复杂度不够，所造成的现象为欠拟合。
- 方差主要是由于模型复杂度过高造成的， 所造成的现象是过拟合。

## 如何降低偏差（欠拟合）

1. 加大模型规模（更换其余机器学习算法，神经网络可以增加每层神经元/神经网络层数）：

   > 偏差很高很有可能是因为模型的拟合能力差，对于传统机器学习算法，各个方法的拟合能力不同，选择一个拟合能力更好的算法往往能够得出很好的结果。 对于神经网络（拟合能力最强）而言，通过增加网络层数或增加每层单元数就能够很好的提高模型的拟合能力[3][4][5]。

2. 根据误差分析结果来修改特征： 

   > 我们需要将错误样本分类，判断可能是由于什么原因导致样本失败，在针对分析结果，增加或减少一些特征。

3. 减少或去除正则化： 这可以避免偏差，但会增大方差。

4. 修改模型结构，以适应你的问题：对于不同的问题，不同的模型结构会产生更好的结果，比如在CV中常用CNN，而在NLP领域常用LSTM。

## 如何降低方差（过拟合）

1. 重新分析，清洗数据。 

   > 有时候，造成方差很大的原因往往是由于数据不良造成的，对于深度学习来说，有一个大规模，高质量的数据集是极为重要的。

2. 添加更多的训练数据。

   > 增大训练数据能够往往能够提高模型的泛化能力。可以采用数据增强技术。

3. 加入正则化。

4. 加入提前终止。

   > 意思就是在训练误差变化很慢甚至不变的时候可以停止训练，这项技术可以降低方差，但有可能增大了偏差。 提前终止有助于我们能够在到达最佳拟合附近，避免进入过拟合状态。

5. 通过特征选择减少输入特征的数量和种类。 

   > 显著减少特征数量能够提高模型的泛化能力，但模型的拟合能力会降低，这意味着，该技术可以减小方差，但可能会增大偏差。 不过在深度学习中，我们往往直接将所有特征放入神经网络中，交给算法来选择取舍。

6. 减少模型规模，降低模型复杂度（每层神经元个数/神经网络层数）： **谨慎使用。** 

   > 一般情况下，对于复杂问题如CV或NLP等问题不会降低模型复杂度，而对于简单问题，采用简单模型往往训练速度更快，效果很好。

7. 根据误差分析结果修改输入特征。

8. 修改模型架构，使之更适合你的问题。 一般可以选择简单模型的情况下，不选择复杂模型。

9. 集成学习。



